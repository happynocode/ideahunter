
import "jsr:@supabase/functions-js/edge-runtime.d.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

interface ScraperTask {
  id: number;
  industry_id: number;
  target_date: string;
  batch_id: string;
  retry_count: number;
  max_retries: number;
}

const BATCH_SIZE = 2; // æ¯æ¬¡å¤„ç†2ä¸ªä»»åŠ¡ï¼ˆä¼˜åŒ–åï¼‰
const LOCK_TIMEOUT = 300000; // 5åˆ†é’Ÿé”å®šè¶…æ—¶
const TASK_TIMEOUT = 300000; // 5åˆ†é’Ÿä»»åŠ¡è¶…æ—¶ (ä¼˜åŒ–åç¼©çŸ­ï¼ŒåŸæ¥æ˜¯10åˆ†é’Ÿ)
const MAX_TASK_RETRIES = 3; // æœ€å¤§é‡è¯•æ¬¡æ•°

Deno.serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  const supabaseClient = createClient(
    Deno.env.get('SUPABASE_URL') ?? '',
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
  )

  // ç”Ÿæˆå”¯ä¸€é”ID
  const lockId = `scraper_coordinator_${Date.now()}_${crypto.randomUUID()}`;
  
  try {
    console.log('Scraper Coordinator: Starting task processing...')

    // 1. å®ç°çœŸæ­£çš„åˆ†å¸ƒå¼é”ï¼šåœ¨scrape_tasksè¡¨ä¸­åˆ›å»ºé”è®°å½•
    const lockExpiry = new Date(Date.now() + LOCK_TIMEOUT).toISOString();
    
    // å°è¯•è·å–é” - æ’å…¥é”è®°å½•
    const { error: lockError } = await supabaseClient
      .from('scrape_tasks')
      .insert({
        industry_id: -1, // ç‰¹æ®Šæ ‡è¯†ï¼šé”è®°å½•
        target_date: 'lock',
        status: 'coordinator_lock',
        batch_id: lockId,
        created_at: lockExpiry, // ä½¿ç”¨created_atä½œä¸ºè¿‡æœŸæ—¶é—´
        error_message: 'Scraper coordinator distributed lock'
      });

    // å¦‚æœæ’å…¥å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ´»è·ƒçš„é”
    if (lockError) {
      // æ£€æŸ¥æ˜¯å¦æœ‰æœªè¿‡æœŸçš„é”
      const { data: existingLocks } = await supabaseClient
        .from('scrape_tasks')
        .select('id')
        .eq('status', 'coordinator_lock')
        .eq('industry_id', -1)
        .gte('created_at', new Date().toISOString())
        .limit(1);

      if (existingLocks && existingLocks.length > 0) {
        console.log('Scraper Coordinator: Another instance is already running, skipping...')
        return new Response(
          JSON.stringify({ 
            success: true, 
            message: 'Another coordinator instance is running',
            tasksProcessed: 0
          }),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }
    }

    // 2. æ¸…ç†è¿‡æœŸçš„é”è®°å½•
    await supabaseClient
      .from('scrape_tasks')
      .delete()
      .eq('status', 'coordinator_lock')
      .eq('industry_id', -1)
      .lt('created_at', new Date().toISOString());

    // 3. æ£€æŸ¥å¹¶é‡ç½®è¶…æ—¶ä»»åŠ¡
    console.log('Scraper Coordinator: Checking for timeout tasks...');
    const { data: timeoutTasks, error: timeoutError } = await supabaseClient
      .from('scrape_tasks')
      .select('id, retry_count, max_retries, industry_id, started_at, batch_id')
      .eq('status', 'scraping')
      .lt('started_at', new Date(Date.now() - TASK_TIMEOUT).toISOString());

    if (timeoutError) {
      console.error('Error checking timeout tasks:', timeoutError);
    }

    if (timeoutTasks && timeoutTasks.length > 0) {
      console.log(`âš ï¸ Scraper Coordinator: Found ${timeoutTasks.length} timeout tasks, analyzing...`);
      
      // è¯¦ç»†è®°å½•è¶…æ—¶ä»»åŠ¡ä¿¡æ¯
      timeoutTasks.forEach(task => {
        const timeoutDuration = Date.now() - new Date(task.started_at).getTime();
        const timeoutMinutes = Math.floor(timeoutDuration / 60000);
        console.log(`â° Timeout Task Details:`, {
          taskId: task.id,
          industryId: task.industry_id,
          batchId: task.batch_id,
          startedAt: task.started_at,
          timeoutMinutes: timeoutMinutes,
          retryCount: task.retry_count,
          maxRetries: task.max_retries
        });
      });
      
      // é‡ç½®æœªè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°çš„è¶…æ—¶ä»»åŠ¡
      const retryableTimeoutTasks = timeoutTasks.filter(task => 
        task.retry_count < (task.max_retries || MAX_TASK_RETRIES)
      );
      const failedTimeoutTasks = timeoutTasks.filter(task => 
        task.retry_count >= (task.max_retries || MAX_TASK_RETRIES)
      );
      
      console.log(`ğŸ”„ Retryable timeout tasks: ${retryableTimeoutTasks.length}, Failed timeout tasks: ${failedTimeoutTasks.length}`);
      
      if (retryableTimeoutTasks.length > 0) {
        for (const task of retryableTimeoutTasks) {
          const newRetryCount = task.retry_count + 1;
          console.log(`ğŸ” Retrying timeout task ${task.id} (attempt ${newRetryCount}/${task.max_retries || MAX_TASK_RETRIES})`);
          
          await supabaseClient
            .from('scrape_tasks')
            .update({
              status: 'pending_scrape',
              retry_count: newRetryCount,
              error_message: `Task timeout after ${Math.floor(TASK_TIMEOUT/60000)} minutes, retrying... (attempt ${newRetryCount})`,
              started_at: null
            })
            .eq('id', task.id);
        }
      }
      
      if (failedTimeoutTasks.length > 0) {
        console.log(`âŒ Marking ${failedTimeoutTasks.length} tasks as failed due to max retries exceeded`);
        
        await supabaseClient
          .from('scrape_tasks')
          .update({
            status: 'failed',
            error_message: `Task timeout after multiple retries. Max timeout: ${Math.floor(TASK_TIMEOUT/60000)} minutes, Max retries: ${MAX_TASK_RETRIES}`,
            completed_at: new Date().toISOString()
          })
          .in('id', failedTimeoutTasks.map(t => t.id));
      }
    }

    // 4. æ£€æŸ¥å¹¶é‡è¯•å¤±è´¥çš„ä»»åŠ¡ï¼ˆ30åˆ†é’Ÿåé‡è¯•ï¼Œä¼˜åŒ–é—´éš”ï¼‰
    console.log('Scraper Coordinator: Checking for retryable failed tasks...');
    const { data: retryTasks } = await supabaseClient
      .from('scrape_tasks')
      .select('*')
      .eq('status', 'failed')
      .filter('retry_count', 'lt', MAX_TASK_RETRIES)
      .lt('completed_at', new Date(Date.now() - 1800000).toISOString()); // 30åˆ†é’Ÿåé‡è¯• (ä¼˜åŒ–é—´éš”)

    if (retryTasks && retryTasks.length > 0) {
      console.log(`ğŸ”„ Scraper Coordinator: Found ${retryTasks.length} retryable failed tasks`);
      
      for (const task of retryTasks) {
        const newRetryCount = task.retry_count + 1;
        console.log(`ğŸ” Retrying failed task ${task.id} (industry ${task.industry_id}, attempt ${newRetryCount}/${MAX_TASK_RETRIES})`);
        
        await supabaseClient
          .from('scrape_tasks')
          .update({
            status: 'pending_scrape',
            retry_count: newRetryCount,
            error_message: `Retrying failed task (attempt ${newRetryCount}/${MAX_TASK_RETRIES})`,
            started_at: null,
            completed_at: null
          })
          .eq('id', task.id);
      }
    }

    // 5. æ£€æŸ¥æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡æ•°é‡
    console.log('Scraper Coordinator: Checking for running scraping tasks...');
    const { data: runningTasks, error: runningError } = await supabaseClient
      .from('scrape_tasks')
      .select('id')
      .eq('status', 'scraping')
      .gt('industry_id', 0); // æ’é™¤é”è®°å½•

    if (runningError) {
      throw new Error(`Failed to check running tasks: ${runningError.message}`);
    }

    const runningTaskCount = runningTasks?.length || 0;
    console.log(`Scraper Coordinator: Found ${runningTaskCount} running scraping tasks`);

    // å¦‚æœå·²ç»æœ‰2ä¸ªæˆ–æ›´å¤šä»»åŠ¡åœ¨è¿è¡Œï¼Œä¸å¤„ç†æ–°ä»»åŠ¡
    if (runningTaskCount >= 2) {
      console.log('Scraper Coordinator: Maximum concurrent tasks reached, skipping new task processing')
      return new Response(
        JSON.stringify({ 
          success: true, 
          message: `Maximum concurrent tasks reached (${runningTaskCount}/2), skipping new task processing`,
          tasksProcessed: 0,
          runningTasks: runningTaskCount
        }),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // 6. è·å–å¾…å¤„ç†çš„ä»»åŠ¡ï¼Œæ ¹æ®å‰©ä½™å®¹é‡é™åˆ¶
    const availableSlots = 2 - runningTaskCount;
    const taskLimit = Math.min(BATCH_SIZE, availableSlots);
    
    console.log(`Scraper Coordinator: Available slots: ${availableSlots}, will process up to ${taskLimit} tasks`);
    
    const { data: pendingTasks, error: fetchError } = await supabaseClient
      .from('scrape_tasks')
      .select(`
        id,
        industry_id,
        target_date,
        batch_id,
        retry_count,
        max_retries
      `)
      .eq('status', 'pending_scrape')
      .gt('industry_id', 0) // æ’é™¤é”è®°å½•
      .order('created_at', { ascending: true })
      .limit(taskLimit);

    if (fetchError) {
      throw new Error(`Failed to fetch pending tasks: ${fetchError.message}`);
    }

    if (!pendingTasks || pendingTasks.length === 0) {
      console.log('Scraper Coordinator: No pending tasks found')
      return new Response(
        JSON.stringify({ 
          success: true, 
          message: 'No pending tasks to process',
          tasksProcessed: 0,
          runningTasks: runningTaskCount
        }),
        { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    console.log(`Scraper Coordinator: Found ${pendingTasks.length} pending tasks, will process with ${runningTaskCount} already running`);

    // 7. ä½¿ç”¨äº‹åŠ¡æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸º 'scraping'
    const taskIds = pendingTasks.map(task => task.id);
    const { error: updateError } = await supabaseClient
      .from('scrape_tasks')
      .update({
        status: 'scraping',
        started_at: new Date().toISOString()
      })
      .in('id', taskIds)
      .eq('status', 'pending_scrape'); // æ·»åŠ çŠ¶æ€æ£€æŸ¥é˜²æ­¢ç«äº‰æ¡ä»¶

    if (updateError) {
      throw new Error(`Failed to update task status: ${updateError.message}`);
    }

    // 8. å‡†å¤‡è°ƒç”¨ reddit-scraper çš„å‚æ•°
    const industryIds = pendingTasks.map(task => task.industry_id);
    const batchIds = [...new Set(pendingTasks.map(task => task.batch_id))];
    const targetDate = pendingTasks[0].target_date;

    const scraperPayload = {
      industry_ids: industryIds,
      target_date: targetDate,
      task_ids: taskIds,
      batch_id: batchIds[0]
    };

    console.log('Scraper Coordinator: Calling reddit-scraper with payload:', {
      ...scraperPayload,
      industry_ids: scraperPayload.industry_ids.length + ' industries',
      task_ids: scraperPayload.task_ids.length + ' tasks'
    });

    // 9. è§¦å‘ reddit-scraper å‡½æ•° (æ”¹è¿›é”™è¯¯å¤„ç†)
    console.log('Scraper Coordinator: Triggering reddit-scraper...');
    
    fetch(`${Deno.env.get('SUPABASE_URL')}/functions/v1/reddit-scraper`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(scraperPayload)
    }).then(async response => {
      if (response.ok) {
        console.log('Scraper Coordinator: Successfully triggered reddit-scraper');
      } else {
        const errorText = await response.text().catch(() => 'Unknown error');
        console.error(`Scraper Coordinator: Failed to trigger reddit-scraper: ${response.status} - ${errorText}`);
        
        // å¦‚æœè§¦å‘å¤±è´¥ï¼Œé‡ç½®ä»»åŠ¡çŠ¶æ€å¹¶è®°å½•é”™è¯¯
        await supabaseClient
          .from('scrape_tasks')
          .update({
            status: 'pending_scrape',
            error_message: `Failed to trigger reddit-scraper: ${response.status} - ${errorText}`,
            started_at: null
          })
          .in('id', taskIds);
      }
    }).catch(async error => {
      console.error('Scraper Coordinator: Error triggering reddit-scraper:', error);
      
      // ç½‘ç»œé”™è¯¯æˆ–å…¶ä»–å¼‚å¸¸ï¼Œé‡ç½®ä»»åŠ¡çŠ¶æ€
      await supabaseClient
        .from('scrape_tasks')
        .update({
          status: 'pending_scrape',
          error_message: `Error triggering reddit-scraper: ${error.message}`,
          started_at: null
        })
        .in('id', taskIds);
    });
    
    console.log('Scraper Coordinator: Reddit scraper triggered, tasks handed off');

    return new Response(
      JSON.stringify({
        success: true,
        message: `Successfully triggered ${pendingTasks.length} scraping tasks`,
        tasksProcessed: pendingTasks.length,
        timeoutTasksReset: (timeoutTasks?.filter(t => t.retry_count < (t.max_retries || MAX_TASK_RETRIES)).length) || 0,
        timeoutTasksFailed: (timeoutTasks?.filter(t => t.retry_count >= (t.max_retries || MAX_TASK_RETRIES)).length) || 0,
        failedTasksRetried: retryTasks?.length || 0,
        runningTasksBefore: runningTaskCount,
        totalRunningAfter: runningTaskCount + pendingTasks.length,
        optimizedTimeoutMinutes: Math.floor(TASK_TIMEOUT / 60000),
        maxRetries: MAX_TASK_RETRIES
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );

  } catch (error) {
    console.error('Scraper Coordinator Error:', error);
    
    return new Response(
      JSON.stringify({
        success: false,
        message: 'Scraper coordinator failed',
        error: error.message,
        tasksProcessed: 0
      }),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 500
      }
    );
  } finally {
    // 10. é‡Šæ”¾é” - åˆ é™¤é”è®°å½•
    try {
      await supabaseClient
        .from('scrape_tasks')
        .delete()
        .eq('batch_id', lockId)
        .eq('status', 'coordinator_lock');
    } catch (error) {
      console.error('Failed to release lock:', error);
    }
  }
}) 